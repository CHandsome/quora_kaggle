{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "sample_submission.csv\n",
      "stanfordData_train1.nlp\n",
      "test.csv\n",
      "test_collins_duffy\n",
      "test_dean\n",
      "train.csv\n",
      "train_collins_duffy\n",
      "train_dean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string\n",
    "import matplotlib.pylab as plt\n",
    "from numpy import linalg as LA\n",
    "%matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform, Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of base training File =  (404290, 6)\n",
      "Shape of base training data after cleaning =  (404290, 6)\n",
      "Shape of base training File =  (2345796, 3)\n",
      "Shape of base training data after cleaning =  (2345796, 3)\n",
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_data(path_to_file):\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    print (\"Shape of base training File = \", df.shape)\n",
    "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
    "    return df\n",
    "\n",
    "df_train = read_data(\"input/train.csv\")\n",
    "df_submit = read_data(\"input/test.csv\")\n",
    "\n",
    "# Print the column names\n",
    "print (df_train.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Collins Duffy Training Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_1e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_1e-1_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_1e-2_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_1e-2_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_1e0_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_1e0_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_2e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_2e-1_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_5e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_5e-1_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_5e-2_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_5e-2_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_8e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/train_collins_duffy/sd_8e-1_st.csv\n"
     ]
    }
   ],
   "source": [
    "df_features = None\n",
    "flip = True\n",
    "base = 'input/train_collins_duffy/'\n",
    "count = 0\n",
    "for filename in os.listdir(base):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        tmpFrame = pd.read_csv(os.path.join(os.getcwd(), base, filename))\n",
    "        print(os.path.join(os.getcwd(), base, filename))\n",
    "        tmpFrame = tmpFrame.rename(columns={'cdNorm_st': filename.replace(\".csv\",\"\")})\n",
    "        if flip:\n",
    "            df_features = tmpFrame\n",
    "            flip = False\n",
    "        else:\n",
    "            df_features = df_features.merge(tmpFrame,how='inner',on='id')\n",
    "        count+=1\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Collins Duffy Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_1e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_1e-1_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_1e-2_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_1e-2_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_1e0_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_1e0_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_2e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_2e-1_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_5e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_5e-1_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_5e-2_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_5e-2_st.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_8e-1_sst.csv\n",
      "/Users/deanfulgoni/Documents/Code/quora_kaggle/input/test_collins_duffy/sd_8e-1_st.csv\n"
     ]
    }
   ],
   "source": [
    "df_features_test = None\n",
    "flip = True\n",
    "base = 'input/test_collins_duffy/'\n",
    "count = 0\n",
    "for filename in os.listdir(base):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        tmpFrame = pd.read_csv(os.path.join(os.getcwd(), base, filename))\n",
    "        print(os.path.join(os.getcwd(), base, filename))\n",
    "        tmpFrame = tmpFrame.rename(columns={'cdNorm_st': filename.replace(\".csv\",\"\")})\n",
    "        if flip:\n",
    "            df_features_test = tmpFrame\n",
    "            flip = False\n",
    "        else:\n",
    "            df_features_test = df_features_test.merge(tmpFrame,how='inner',on='id')\n",
    "        count+=1\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Dean's Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_semantic = pd.read_csv('input/train_dean/dean_train_features.csv')\n",
    "df_semantic.drop('is_duplicate',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, auc, roc_curve, log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Read in the pre-built features for the training set\n",
    "df_train = df_train.merge(df_features,how='inner',on='id')\n",
    "df_train = df_train.merge(df_semantic,how='inner',on='id')\n",
    "\n",
    "# Methodology - fill with zeroes Any row that crashed the Stanford CoreNLP parser must have been empty string\n",
    "df_train.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate',\n",
       "       'sd_1e-1_sst', 'sd_1e-1_st', 'sd_1e-2_sst', 'sd_1e-2_st', 'sd_1e0_sst',\n",
       "       'sd_1e0_st', 'sd_2e-1_sst', 'sd_2e-1_st', 'sd_5e-1_sst', 'sd_5e-1_st',\n",
       "       'sd_5e-2_sst', 'sd_5e-2_st', 'sd_8e-1_sst', 'sd_8e-1_st', 'S_sem_sim',\n",
       "       'T_sem_sim', 'sem_sim', 'S_unmatch_n', 'T_unmatch_n', 'S_unmatch_n_p',\n",
       "       'T_unmatch_n_p', 'S_unmatch_a', 'T_unmatch_a', 'S_unmatch_a_p',\n",
       "       'T_unmatch_a_p', 'S_unmatch_v', 'T_unmatch_v', 'S_unmatch_v_p',\n",
       "       'T_unmatch_v_p', 'S_unmatch_pp', 'T_unmatch_pp', 'S_unmatch_pp_p',\n",
       "       'T_unmatch_pp_p', 'S_unmatch_wp', 'T_unmatch_wp', 'S_unmatch_wp_p',\n",
       "       'T_unmatch_wp_p', 'S_unmatch_num', 'T_unmatch_num', 'S_unmatch_num_p',\n",
       "       'T_unmatch_num_p', 'S_unmatch_ner', 'T_unmatch_ner', 'len_dif',\n",
       "       'len_dif_p'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:models/model_v3.0.ipynb
=======
    "## Adding More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Question Lengths\n",
    "# df_train['q1len'] = df_train['question1'].str.len()\n",
    "# df_train['q2len'] = df_train['question2'].str.len()\n",
    "\n",
    "# df_submit['q1len'] = df_submit['question1'].str.len()\n",
    "# df_submit['q2len'] = df_submit['question2'].str.len()\n",
    "\n",
    "# # Normalized Word Share\n",
    "# def number_of_words(row):\n",
    "#     try:\n",
    "#         nwords = len(row.split(\" \"))\n",
    "#         return nwords\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# # Normalized Word Share\n",
    "# def normalized_word_share(row):\n",
    "#     try:\n",
    "#         w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "#         w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "#         return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# def word_diff(row):\n",
    "#     try:\n",
    "#         diff = abs(row['q1_n_words']-row['q2_n_words'])\n",
    "#         return diff\n",
    "#     except:\n",
    "#         return 0\n",
    "    \n",
    "# # Number of Words\n",
    "# df_train['q1_n_words'] = df_train['question1'].apply(number_of_words)\n",
    "# df_train['q2_n_words'] = df_train['question2'].apply(number_of_words)\n",
    "\n",
    "# df_submit['q1_n_words'] = df_submit['question1'].apply(number_of_words)\n",
    "# df_submit['q2_n_words'] = df_submit['question2'].apply(number_of_words)\n",
    "    \n",
    "# # Normalized Word share\n",
    "# df_train['word_share'] = df_train.apply(normalized_word_share, axis=1)\n",
    "# df_submit['word_share'] = df_submit.apply(normalized_word_share, axis=1)\n",
    "\n",
    "# df_train['word_diff'] = df_train.apply(word_diff, axis=1)\n",
    "# df_submit['word_diff'] = df_submit.apply(word_diff, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select_cols = list(df_train.columns[6:])\n",
    "df_y = df_train['is_duplicate']\n",
    "df_X = df_train[select_cols].copy()\n",
    "\n",
    "df_X.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404282, 45), (404282,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_X.values\n",
    "y = df_y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.10, random_state=24)\n",
    "\n",
    "X_train_mat = X_train.values\n",
    "y_train_mat = y_train.values\n",
    "\n",
    "X_train_mat = MinMaxScaler().fit_transform(X_train_mat)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=3)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "column_indexer = selector.get_support()\n",
    "X_df_selected = df_X.loc[:,column_indexer]\n",
    "X_df_selected.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 0]['sem_sim'], bins=20, normed=False, label='Not Duplicate')\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 1]['sem_sim'], bins=20, normed=False, alpha=0.6, label='Duplicate')\n",
    "plt.legend()\n",
    "plt.title('Label Distribution Over Semantic Similarity of Aligned Words', fontsize=15)\n",
    "plt.xlabel('Normalized Percentage of Aligned Pairs', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 0]['sd_1e-1_sst'], bins=20, normed=False, label='Not Duplicate')\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 1]['sd_1e-1_sst'], bins=20, normed=False, alpha=0.6, label='Duplicate')\n",
    "plt.legend()\n",
    "plt.title('Label Distribution Over SST Tree Kernel, Lambda = 0.1', fontsize=15)\n",
    "plt.xlabel('Normalized Score', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 0]['S_unmatch_ner'], bins=20, normed=False, label='Not Duplicate')\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 1]['S_unmatch_ner'], bins=20, normed=False, alpha=0.6, label='Duplicate')\n",
    "plt.legend()\n",
    "plt.title('Label Distribution Over Number Unmatched Named Entities', fontsize=15)\n",
    "plt.xlabel('Number Unmatched Named Entities (cutoff=3)', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grid = {\n",
    " #   'C': [1e-3, 1e0, 1e1],\n",
    "#    'penalty': ['l1', 'l2']\n",
    "#}\n",
    "#logreg_cv = GridSearchCV(logreg_clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Print the best cross-validation parameters\n",
    "#print(logreg_cv.best_params_)\n",
    "#print(logreg_cv.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Best was found to be l2 penalty (default) and C=10\n",
    "logreg_clf = LogisticRegression(C=10)\n",
    "y_pred_logreg = cross_val_predict(logreg_clf, X, y)\n",
    "log_loss_scores = cross_val_score(logreg_clf, X, y, scoring='neg_log_loss')\n",
    "ll = log_loss_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y, y_pred_logreg)\n",
    "prec = precision_score(y, y_pred_logreg)\n",
    "recall = recall_score(y, y_pred_logreg)\n",
    "f1 = f1_score(y, y_pred_logreg)\n",
    "print(acc, prec, recall, f1)\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_features': [5, 10, 20]\n",
    "}\n",
    "rf_cv = GridSearchCV(rf_clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\n",
    "rf_cv.fit(X_train_mat, y_train_mat)\n",
    "print(rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=200, max_features=5)\n",
    "y_pred_rf = cross_val_predict(rf_clf, X, y)\n",
    "log_loss_scores = cross_val_score(rf_clf, X, y, scoring='neg_log_loss')\n",
    "ll = log_loss_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y, y_pred_rf)\n",
    "prec = precision_score(y, y_pred_rf)\n",
    "recall = recall_score(y, y_pred_rf)\n",
    "f1 = f1_score(y, y_pred_rf)\n",
    "print(acc, prec, recall, f1)\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1e-06, parameters [[ -4.02694923e-03  -1.76237985e-03  -4.32947573e-03  -1.83683510e-03\n",
      "   -1.16675636e-03  -2.74875372e-04  -3.62707644e-03  -1.67419565e-03\n",
      "   -1.62922762e-03  -1.33967680e-03  -4.20072525e-03  -1.80413582e-03\n",
      "   -1.11528606e-03  -8.42593049e-04  -1.69732036e-06  -5.76649337e-05\n",
      "    8.50075313e-05  -4.93404876e-03  -2.30567130e-03  -3.63872821e-02\n",
      "   -3.53036975e-02  -3.39068679e-03  -2.55325751e-03  -2.15265240e-02\n",
      "   -2.14493952e-02  -4.99722007e-03  -4.52765502e-03  -3.05029229e-02\n",
      "   -3.08046198e-02  -1.19343903e-03  -1.80584390e-03  -1.14720677e-02\n",
      "   -1.36367353e-02  -3.00718789e-03  -2.49787586e-03  -8.75038514e-03\n",
      "   -7.30146023e-03  -4.28548457e-04  -4.24322365e-04  -5.89364587e-03\n",
      "   -6.33703880e-03  -1.94813454e-03  -1.82359187e-03  -1.65267204e-03\n",
      "   -9.55055186e-03]] and intercept [-0.036153]\n",
      "C: 0.001, parameters [[ 0.07135889  0.09322024  0.13440348  0.04339927 -0.56313248  0.23156951\n",
      "  -0.0277675   0.14392116 -0.5052481   0.2697901   0.10894111  0.06607235\n",
      "  -0.69657887  0.31745091  0.45528966  0.43620946  0.44203754 -0.46434797\n",
      "  -0.21495258 -1.19545639 -1.05566629 -0.14939504 -0.09869071 -0.25694829\n",
      "  -0.26223847 -0.08383074 -0.11254316 -0.1090087  -0.1559611  -0.07040174\n",
      "  -0.116176    0.04156966 -0.03573359 -0.01605826  0.00584672 -0.02601846\n",
      "   0.00934398 -0.02647812 -0.03576121 -0.24908616 -0.2843034  -0.27061568\n",
      "  -0.27432842 -0.10994557 -0.40325427]] and intercept [ 0.00168149]\n",
      "C: 1.0, parameters [[ -1.49546215e+00  -2.27259438e-01   4.38226124e+00  -3.72031694e+00\n",
      "    2.62109986e+00  -3.93699455e-01  -3.96462501e+00   2.03838660e+00\n",
      "   -4.30916297e-01   3.14255917e+00   1.39815482e+00  -1.97137004e+00\n",
      "   -3.78870479e+00   1.31960895e+00   4.25534917e+00   3.98931745e+00\n",
      "    7.42028955e-01  -6.22956041e+00  -9.95501219e+00  -5.06343795e-01\n",
      "   -4.26781121e-01  -1.45028817e+00  -1.11461454e+00   1.17720311e-02\n",
      "   -6.12214235e-02   7.69597583e-01   3.82499606e-01  -1.06333424e-01\n",
      "   -1.16634712e-01  -4.87320972e+00  -4.44767374e+00   4.88402662e-01\n",
      "    4.84568874e-01  -3.16919185e-01   3.15758962e-01   1.91609171e-01\n",
      "    8.28960420e-03   1.99562172e-01  -2.02002939e+00  -1.30045672e-01\n",
      "    7.08230681e-03  -1.45140757e+00  -2.59095922e+00   4.47310769e+00\n",
      "   -8.85768094e-01]] and intercept [-0.93441423]\n",
      "C: 10.0, parameters [[ -7.82364154e+00   2.46798283e+00   1.04326422e+01  -8.09662493e+00\n",
      "    2.86238109e+00  -3.82701394e-01  -2.22765026e+00   6.49633514e+00\n",
      "   -5.56428885e-02   1.88335485e-01  -2.38147091e-01  -2.41328718e+00\n",
      "   -4.15712203e+00   1.90374231e+00   6.62786067e+00   6.21046987e+00\n",
      "   -4.07981770e+00  -6.44443637e+00  -1.16278961e+01  -4.95791743e-01\n",
      "   -3.60776252e-01  -1.52554305e+00  -1.20942123e+00   1.86018066e-02\n",
      "   -5.68118471e-02   7.30938933e-01   3.37027671e-01  -1.02553327e-01\n",
      "   -1.16392352e-01  -5.41365231e+00  -4.78591848e+00   5.31513313e-01\n",
      "    5.14958046e-01  -3.75284758e-01   3.08119397e-01   2.13099021e-01\n",
      "    1.06026635e-02   1.68565426e-01  -2.85008043e+00  -1.34275518e-01\n",
      "    3.98318803e-02  -1.42761785e+00  -2.44062226e+00   6.67954799e+00\n",
      "   -1.35653666e+00]] and intercept [-0.89676698]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1464c6da0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'y', 'k', 'c', 'm', 'brown', 'r']\n",
    "lw = 1\n",
    "Cs = [1e-6, 1e-3, 1e0, 1e1]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for different classifiers')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "labels = []\n",
    "for idx, C in enumerate(Cs):\n",
    "    clf = LogisticRegression(C = C)\n",
    "    clf.fit(X_train_mat, y_train_mat)\n",
    "    print(\"C: {}, parameters {} and intercept {}\".format(C, clf.coef_, clf.intercept_))\n",
    "    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=colors[idx])\n",
    "    labels.append(\"C: {}, AUC = {}\".format(C, np.round(roc_auc, 4)))\n",
    "\n",
    "plt.legend(['random AUC = 0.5'] + labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x13d78b2e8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pr, re, _ = precision_recall_curve(y_test, cv.best_estimator_.predict_proba(X_test)[:,1])\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(re, pr)\n",
    "plt.title('PR Curve (AUC {})'.format(auc(re, pr)))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in test set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_semantic_test = pd.read_csv('input/test_dean/dean_test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the pre-built features for the test set\n",
    "df_submit = df_submit.merge(df_features_test,how='left',left_on='test_id',right_on='id')\n",
    "# df_train = df_train.merge(df_semantic,how='inner',on='id')\n",
    "df_submit.drop(['id'],inplace=True,axis=1)\n",
    "df_submit = df_submit.merge(df_semantic_test,how='left',left_on='test_id',right_on='id')\n",
    "df_submit.drop(['id'],inplace=True,axis=1)\n",
    "\n",
    "# Methodology - fill with zeroes Any row that crashed the Stanford CoreNLP parser must have been empty string\n",
    "df_submit.fillna(value=0, inplace=True)\n",
    "\n",
    "X_test_set = df_submit[df_submit.columns[3:]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions for Logistic Regression and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_clf = LogisticRegression(C=10)\n",
    "logreg_clf.fit(X, y)\n",
    "probs = logreg_clf.predict_proba(X_test_set)\n",
    "df_submit['is_duplicate'] = probs[:,1]\n",
    "df_final_submit = df_submit[['test_id','is_duplicate']]\n",
    "df_final_submit.to_csv(\"submissions/lr_final.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=200, max_features=5)\n",
    "rf_clf.fit(X, y)\n",
    "probs = rf_clf.predict_proba(X_test_set)\n",
    "df_submit['is_duplicate'] = probs[:,1]\n",
    "df_final_submit = df_submit[['test_id','is_duplicate']]\n",
    "df_final_submit.to_csv(\"submissions/rf_final.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sd_1e-1_sst', 'sd_1e-1_st', 'sd_1e-2_sst', 'sd_1e-2_st', 'sd_1e0_sst',\n",
       "       'sd_1e0_st', 'sd_2e-1_sst', 'sd_2e-1_st', 'sd_5e-1_sst', 'sd_5e-1_st',\n",
       "       'sd_5e-2_sst', 'sd_5e-2_st', 'sd_8e-1_sst', 'sd_8e-1_st', 'S_sem_sim',\n",
       "       'T_sem_sim', 'sem_sim', 'S_unmatch_n', 'T_unmatch_n', 'S_unmatch_n_p',\n",
       "       'T_unmatch_n_p', 'S_unmatch_a', 'T_unmatch_a', 'S_unmatch_a_p',\n",
       "       'T_unmatch_a_p', 'S_unmatch_v', 'T_unmatch_v', 'S_unmatch_v_p',\n",
       "       'T_unmatch_v_p', 'S_unmatch_pp', 'T_unmatch_pp', 'S_unmatch_pp_p',\n",
       "       'T_unmatch_pp_p', 'S_unmatch_wp', 'T_unmatch_wp', 'S_unmatch_wp_p',\n",
       "       'T_unmatch_wp_p', 'S_unmatch_num', 'T_unmatch_num', 'S_unmatch_num_p',\n",
       "       'T_unmatch_num_p', 'S_unmatch_ner', 'T_unmatch_ner', 'len_dif',\n",
       "       'len_dif_p'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x123d5d2b0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 0]['S_sem_sim'], bins=20, normed=True, label='Not Duplicate')\n",
    "plt.hist(df_X[df_train['is_duplicate'] == 1]['S_sem_sim'], bins=20, normed=True, alpha=0.7, label='Duplicate')\n",
    "plt.legend()\n",
    "plt.title('Label distribution over Semantic Similarity of Aligned Words', fontsize=15)\n",
    "plt.xlabel('normalized % aligned pairs', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
