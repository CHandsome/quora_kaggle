{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP \n",
    "import re, time, bisect, math\n",
    "import TreeBuild as tb\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of base training File =  (404290, 6)\n",
      "Shape of base training data after cleaning =  (404290, 6)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def read_data(path_to_file):\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    print (\"Shape of base training File = \", df.shape)\n",
    "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
    "    return df\n",
    "\n",
    "df_train = read_data(\"input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _getNLPToks_(rawSentence):\n",
    "    try:\n",
    "        output = nlp.annotate(rawSentence, properties={\n",
    "            'annotators': 'tokenize,ssplit,pos,parse,depparse',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "    except UnicodeDecodeError:\n",
    "        sentence = unidecode(rawSentence)\n",
    "        output = nlp.annotate(sentence, properties={\n",
    "            'annotators': 'tokenize,ssplit,pos,parse,depparse',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "    dependencies = output['sentences'][0]['basicDependencies']\n",
    "    tokens = output['sentences'][0]['tokens']\n",
    "    parse = output['sentences'][0]['parse'].split(\"\\n\")\n",
    "    return {'deps':dependencies, 'toks':tokens, 'parse':parse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import json, os\n",
    "with open('stanfordData', 'wb') as fout:\n",
    "    count = 0\n",
    "    for row in df_train.iterrows():\n",
    "        count += 1\n",
    "        if count > 5:\n",
    "            break\n",
    "        \n",
    "        q1_stanford = _getNLPToks_(row[1]['question1'])\n",
    "        q2_stanford = _getNLPToks_(row[1]['question2'])\n",
    "\n",
    "        tree_1 = tb.tree()\n",
    "        tree_2 = tb.tree()\n",
    "\n",
    "        # Generate a tree structure\n",
    "        tb._generateTree_(q1_stanford['parse'], tree_1)\n",
    "        tb._generateTree_(q2_stanford['parse'], tree_2)\n",
    "\n",
    "        # Flip the trees\n",
    "        tb._flipTree_(tree_1)\n",
    "        tb._flipTree_(tree_2)\n",
    "\n",
    "        tmp = {'q1': {\n",
    "                'raw': row[1]['question1'],\n",
    "                'toks': q1_stanford['toks'],\n",
    "                'deps': q1_stanford['deps'],\n",
    "                'parse': tree_1\n",
    "                },\n",
    "               'q2': {\n",
    "                'raw': row[1]['question2'],\n",
    "                'toks': q2_stanford['toks'],\n",
    "                'deps': q2_stanford['deps'],\n",
    "                'parse': tree_2                \n",
    "                },\n",
    "               'id':row[1]['id'],\n",
    "               'is_duplicate':row[1]['is_duplicate']\n",
    "               }\n",
    "               \n",
    "        pickle.dump(tmp, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        tree_1.clear()\n",
    "        tree_2.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india?\n",
      "What is the step by step guide to invest in share market?\n",
      "What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n",
      "How can I increase the speed of my internet connection while using a VPN?\n",
      "How can Internet speed be increased by hacking through DNS?\n",
      "Why am I mentally very lonely? How can I solve it?\n",
      "Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?\n",
      "Which fish would survive in salt water?\n"
     ]
    }
   ],
   "source": [
    "with open('stanfordData', 'rb') as handle:\n",
    "    try:\n",
    "        while True:\n",
    "            tmp = pickle.load(handle)\n",
    "            print(tmp['q1']['raw'])\n",
    "            print(tmp['q2']['raw'])\n",
    "\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
