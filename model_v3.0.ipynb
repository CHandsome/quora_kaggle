{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "QuoraKaggle.pem\n",
      "collins_duffy_test.zip\n",
      "collins_duffy_train.zip\n",
      "data_dump.zip\n",
      "ec2-user@ec2-52-91-73-35.compute-1.amazonaws.com\n",
      "sample_submission.csv\n",
      "semantic_pos_features.csv\n",
      "stanfordData_train.nlp\n",
      "stanfordData_train_ner.nlp\n",
      "test.csv\n",
      "test_collins_duffy\n",
      "test_preprocessed\n",
      "test_raw\n",
      "train.csv\n",
      "train_collins_duffy\n",
      "train_dean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string\n",
    "import matplotlib.pylab as plt\n",
    "from numpy import linalg as LA\n",
    "%matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform, Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of base training File =  (404290, 6)\n",
      "Shape of base training data after cleaning =  (404290, 6)\n",
      "Shape of base training File =  (2345796, 3)\n",
      "Shape of base training data after cleaning =  (2345796, 3)\n",
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_data(path_to_file):\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    print (\"Shape of base training File = \", df.shape)\n",
    "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
    "    return df\n",
    "\n",
    "df_train = read_data(\"input/train.csv\")\n",
    "df_submit = read_data(\"input/test.csv\")\n",
    "\n",
    "# Print the column names\n",
    "print (df_train.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Collins Duffy Training Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_1e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_1e-1_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_1e-2_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_1e-2_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_1e0_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_1e0_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_2e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_2e-1_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_5e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_5e-1_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_5e-2_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_5e-2_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_8e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/train_collins_duffy/sd_8e-1_st.csv\n"
     ]
    }
   ],
   "source": [
    "df_features = None\n",
    "flip = True\n",
    "base = 'input/train_collins_duffy/'\n",
    "count = 0\n",
    "for filename in os.listdir(base):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        tmpFrame = pd.read_csv(os.path.join(os.getcwd(), base, filename))\n",
    "        print(os.path.join(os.getcwd(), base, filename))\n",
    "        tmpFrame = tmpFrame.rename(columns={'cdNorm_st': filename.replace(\".csv\",\"\")})\n",
    "        if flip:\n",
    "            df_features = tmpFrame\n",
    "            flip = False\n",
    "        else:\n",
    "            df_features = df_features.merge(tmpFrame,how='inner',on='id')\n",
    "        count+=1\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Collins Duffy Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_1e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_1e-1_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_1e-2_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_1e-2_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_1e0_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_1e0_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_2e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_2e-1_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_5e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_5e-1_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_5e-2_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_5e-2_st.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_8e-1_sst.csv\n",
      "/Users/Matthew/Documents/CIS700/FinalProject_Kaggle/input/test_collins_duffy/sd_8e-1_st.csv\n"
     ]
    }
   ],
   "source": [
    "df_features_test = None\n",
    "flip = True\n",
    "base = 'input/test_collins_duffy/'\n",
    "count = 0\n",
    "for filename in os.listdir(base):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        tmpFrame = pd.read_csv(os.path.join(os.getcwd(), base, filename))\n",
    "        print(os.path.join(os.getcwd(), base, filename))\n",
    "        tmpFrame = tmpFrame.rename(columns={'cdNorm_st': filename.replace(\".csv\",\"\")})\n",
    "        if flip:\n",
    "            df_features_test = tmpFrame\n",
    "            flip = False\n",
    "        else:\n",
    "            df_features_test = df_features_test.merge(tmpFrame,how='inner',on='id')\n",
    "        count+=1\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Dean's Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_semantic = pd.read_csv('input/train_dean/dean_train_features.csv')\n",
    "df_semantic.drop('is_duplicate',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Read in the pre-built features for the training set\n",
    "df_train = df_train.merge(df_features,how='inner',on='id')\n",
    "df_train = df_train.merge(df_semantic,how='inner',on='id')\n",
    "\n",
    "# Methodology - fill with zeroes Any row that crashed the Stanford CoreNLP parser must have been empty string\n",
    "df_train.fillna(value=0, inplace=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate',\n",
       "       'sd_1e-1_sst', 'sd_1e-1_st', 'sd_1e-2_sst', 'sd_1e-2_st', 'sd_1e0_sst',\n",
       "       'sd_1e0_st', 'sd_2e-1_sst', 'sd_2e-1_st', 'sd_5e-1_sst', 'sd_5e-1_st',\n",
       "       'sd_5e-2_sst', 'sd_5e-2_st', 'sd_8e-1_sst', 'sd_8e-1_st', 'S_sem_sim',\n",
       "       'T_sem_sim', 'sem_sim', 'S_unmatch_n', 'T_unmatch_n', 'S_unmatch_n_p',\n",
       "       'T_unmatch_n_p', 'S_unmatch_a', 'T_unmatch_a', 'S_unmatch_a_p',\n",
       "       'T_unmatch_a_p', 'S_unmatch_v', 'T_unmatch_v', 'S_unmatch_v_p',\n",
       "       'T_unmatch_v_p', 'S_unmatch_pp', 'T_unmatch_pp', 'S_unmatch_pp_p',\n",
       "       'T_unmatch_pp_p', 'S_unmatch_wp', 'T_unmatch_wp', 'S_unmatch_wp_p',\n",
       "       'T_unmatch_wp_p', 'S_unmatch_num', 'T_unmatch_num', 'S_unmatch_num_p',\n",
       "       'T_unmatch_num_p', 'S_unmatch_ner', 'T_unmatch_ner', 'len_dif',\n",
       "       'len_dif_p'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Question Lengths\n",
    "# df_train['q1len'] = df_train['question1'].str.len()\n",
    "# df_train['q2len'] = df_train['question2'].str.len()\n",
    "\n",
    "# df_submit['q1len'] = df_submit['question1'].str.len()\n",
    "# df_submit['q2len'] = df_submit['question2'].str.len()\n",
    "\n",
    "# # Normalized Word Share\n",
    "# def number_of_words(row):\n",
    "#     try:\n",
    "#         nwords = len(row.split(\" \"))\n",
    "#         return nwords\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# # Normalized Word Share\n",
    "# def normalized_word_share(row):\n",
    "#     try:\n",
    "#         w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "#         w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "#         return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# def word_diff(row):\n",
    "#     try:\n",
    "#         diff = abs(row['q1_n_words']-row['q2_n_words'])\n",
    "#         return diff\n",
    "#     except:\n",
    "#         return 0\n",
    "    \n",
    "# # Number of Words\n",
    "# df_train['q1_n_words'] = df_train['question1'].apply(number_of_words)\n",
    "# df_train['q2_n_words'] = df_train['question2'].apply(number_of_words)\n",
    "\n",
    "# df_submit['q1_n_words'] = df_submit['question1'].apply(number_of_words)\n",
    "# df_submit['q2_n_words'] = df_submit['question2'].apply(number_of_words)\n",
    "    \n",
    "# # Normalized Word share\n",
    "# df_train['word_share'] = df_train.apply(normalized_word_share, axis=1)\n",
    "# df_submit['word_share'] = df_submit.apply(normalized_word_share, axis=1)\n",
    "\n",
    "# df_train['word_diff'] = df_train.apply(word_diff, axis=1)\n",
    "# df_submit['word_diff'] = df_submit.apply(word_diff, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select_cols = list(df_train.columns[6:])\n",
    "df_y = df_train['is_duplicate']\n",
    "df_X = df_train[select_cols].copy()\n",
    "\n",
    "df_X.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((363853, 45), (40429, 45), (363853,), (40429,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.10, random_state=24)\n",
    "\n",
    "X_train_mat = X_train.values\n",
    "y_train_mat = y_train.values\n",
    "\n",
    "X_train_mat = MinMaxScaler().fit_transform(X_train_mat)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 17.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': [0.001, 1.0, 10.0], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_log_loss', verbose=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "grid = {\n",
    "    'C': [1e-3, 1e0, 1e1],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "cv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\n",
    "cv.fit(X_train_mat, y_train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.0, 'penalty': 'l2'}\n",
      "[[ -7.82364154e+00   2.46798283e+00   1.04326422e+01  -8.09662493e+00\n",
      "    2.86238109e+00  -3.82701394e-01  -2.22765026e+00   6.49633514e+00\n",
      "   -5.56428885e-02   1.88335485e-01  -2.38147091e-01  -2.41328718e+00\n",
      "   -4.15712203e+00   1.90374231e+00   6.62786067e+00   6.21046987e+00\n",
      "   -4.07981770e+00  -6.44443637e+00  -1.16278961e+01  -4.95791743e-01\n",
      "   -3.60776252e-01  -1.52554305e+00  -1.20942123e+00   1.86018066e-02\n",
      "   -5.68118471e-02   7.30938933e-01   3.37027671e-01  -1.02553327e-01\n",
      "   -1.16392352e-01  -5.41365231e+00  -4.78591848e+00   5.31513313e-01\n",
      "    5.14958046e-01  -3.75284758e-01   3.08119397e-01   2.13099021e-01\n",
      "    1.06026635e-02   1.68565426e-01  -2.85008043e+00  -1.34275518e-01\n",
      "    3.98318803e-02  -1.42761785e+00  -2.44062226e+00   6.67954799e+00\n",
      "   -1.35653666e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Print the best cross-validation parameters\n",
    "print(cv.best_params_)\n",
    "print(cv.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1e-06, parameters [[ -4.02694923e-03  -1.76237985e-03  -4.32947573e-03  -1.83683510e-03\n",
      "   -1.16675636e-03  -2.74875372e-04  -3.62707644e-03  -1.67419565e-03\n",
      "   -1.62922762e-03  -1.33967680e-03  -4.20072525e-03  -1.80413582e-03\n",
      "   -1.11528606e-03  -8.42593049e-04  -1.69732036e-06  -5.76649337e-05\n",
      "    8.50075313e-05  -4.93404876e-03  -2.30567130e-03  -3.63872821e-02\n",
      "   -3.53036975e-02  -3.39068679e-03  -2.55325751e-03  -2.15265240e-02\n",
      "   -2.14493952e-02  -4.99722007e-03  -4.52765502e-03  -3.05029229e-02\n",
      "   -3.08046198e-02  -1.19343903e-03  -1.80584390e-03  -1.14720677e-02\n",
      "   -1.36367353e-02  -3.00718789e-03  -2.49787586e-03  -8.75038514e-03\n",
      "   -7.30146023e-03  -4.28548457e-04  -4.24322365e-04  -5.89364587e-03\n",
      "   -6.33703880e-03  -1.94813454e-03  -1.82359187e-03  -1.65267204e-03\n",
      "   -9.55055186e-03]] and intercept [-0.036153]\n",
      "C: 0.001, parameters [[ 0.07135889  0.09322024  0.13440348  0.04339927 -0.56313248  0.23156951\n",
      "  -0.0277675   0.14392116 -0.5052481   0.2697901   0.10894111  0.06607235\n",
      "  -0.69657887  0.31745091  0.45528966  0.43620946  0.44203754 -0.46434797\n",
      "  -0.21495258 -1.19545639 -1.05566629 -0.14939504 -0.09869071 -0.25694829\n",
      "  -0.26223847 -0.08383074 -0.11254316 -0.1090087  -0.1559611  -0.07040174\n",
      "  -0.116176    0.04156966 -0.03573359 -0.01605826  0.00584672 -0.02601846\n",
      "   0.00934398 -0.02647812 -0.03576121 -0.24908616 -0.2843034  -0.27061568\n",
      "  -0.27432842 -0.10994557 -0.40325427]] and intercept [ 0.00168149]\n",
      "C: 1.0, parameters [[ -1.49546215e+00  -2.27259438e-01   4.38226124e+00  -3.72031694e+00\n",
      "    2.62109986e+00  -3.93699455e-01  -3.96462501e+00   2.03838660e+00\n",
      "   -4.30916297e-01   3.14255917e+00   1.39815482e+00  -1.97137004e+00\n",
      "   -3.78870479e+00   1.31960895e+00   4.25534917e+00   3.98931745e+00\n",
      "    7.42028955e-01  -6.22956041e+00  -9.95501219e+00  -5.06343795e-01\n",
      "   -4.26781121e-01  -1.45028817e+00  -1.11461454e+00   1.17720311e-02\n",
      "   -6.12214235e-02   7.69597583e-01   3.82499606e-01  -1.06333424e-01\n",
      "   -1.16634712e-01  -4.87320972e+00  -4.44767374e+00   4.88402662e-01\n",
      "    4.84568874e-01  -3.16919185e-01   3.15758962e-01   1.91609171e-01\n",
      "    8.28960420e-03   1.99562172e-01  -2.02002939e+00  -1.30045672e-01\n",
      "    7.08230681e-03  -1.45140757e+00  -2.59095922e+00   4.47310769e+00\n",
      "   -8.85768094e-01]] and intercept [-0.93441423]\n",
      "C: 10.0, parameters [[ -7.82364154e+00   2.46798283e+00   1.04326422e+01  -8.09662493e+00\n",
      "    2.86238109e+00  -3.82701394e-01  -2.22765026e+00   6.49633514e+00\n",
      "   -5.56428885e-02   1.88335485e-01  -2.38147091e-01  -2.41328718e+00\n",
      "   -4.15712203e+00   1.90374231e+00   6.62786067e+00   6.21046987e+00\n",
      "   -4.07981770e+00  -6.44443637e+00  -1.16278961e+01  -4.95791743e-01\n",
      "   -3.60776252e-01  -1.52554305e+00  -1.20942123e+00   1.86018066e-02\n",
      "   -5.68118471e-02   7.30938933e-01   3.37027671e-01  -1.02553327e-01\n",
      "   -1.16392352e-01  -5.41365231e+00  -4.78591848e+00   5.31513313e-01\n",
      "    5.14958046e-01  -3.75284758e-01   3.08119397e-01   2.13099021e-01\n",
      "    1.06026635e-02   1.68565426e-01  -2.85008043e+00  -1.34275518e-01\n",
      "    3.98318803e-02  -1.42761785e+00  -2.44062226e+00   6.67954799e+00\n",
      "   -1.35653666e+00]] and intercept [-0.89676698]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14792fcf8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = ['r', 'g', 'b', 'y', 'k', 'c', 'm', 'brown', 'r']\n",
    "lw = 1\n",
    "Cs = [1e-6, 1e-3, 1e0, 1e1]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for different classifiers')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "labels = []\n",
    "for idx, C in enumerate(Cs):\n",
    "    clf = LogisticRegression(C = C)\n",
    "    clf.fit(X_train_mat, y_train_mat)\n",
    "    print(\"C: {}, parameters {} and intercept {}\".format(C, clf.coef_, clf.intercept_))\n",
    "    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=colors[idx])\n",
    "    labels.append(\"C: {}, AUC = {}\".format(C, np.round(roc_auc, 4)))\n",
    "\n",
    "plt.legend(['random AUC = 0.5'] + labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2564c62b0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr, re, _ = precision_recall_curve(y_test, cv.best_estimator_.predict_proba(X_test)[:,1])\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(re, pr)\n",
    "plt.title('PR Curve (AUC {})'.format(auc(re, pr)))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in test set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_semantic_test = pd.read_csv('input/test_dean/dean_test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the pre-built features for the test set\n",
    "df_submit = df_submit.merge(df_features_test,how='left',left_on='test_id',right_on='id')\n",
    "# df_train = df_train.merge(df_semantic,how='inner',on='id')\n",
    "df_submit.drop(['id'],inplace=True,axis=1)\n",
    "df_submit = df_submit.merge(df_semantic_test,how='left',left_on='test_id',right_on='id')\n",
    "df_submit.drop(['id'],inplace=True,axis=1)\n",
    "\n",
    "# Methodology - fill with zeroes Any row that crashed the Stanford CoreNLP parser must have been empty string\n",
    "df_submit.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrain on the entire training set\n",
    "retrained = cv.best_estimator_.fit(df_X.values, df_y.values)\n",
    "\n",
    "X_test_set = df_submit[df_submit.columns[3:]].values\n",
    "\n",
    "probs = retrained.predict_proba(df_submit[df_submit.columns[3:]].values)\n",
    "df_submit['is_duplicate'] = probs[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The final submission set\n",
    "df_final_submit = df_submit[['test_id','is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.hist(df_submit['is_duplicate'], 50, normed=1, facecolor='green', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final_submit.to_csv(\"submissions/submission_v4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
